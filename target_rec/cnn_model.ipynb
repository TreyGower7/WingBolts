{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6c90d8-853c-4793-90e5-cb1118557857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 21:07:26.900863: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-14 21:07:27.023605: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-14 21:07:27.564865: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-14 21:07:27.564957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-14 21:07:27.661744: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-14 21:07:27.938435: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-14 21:07:27.952205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-14 21:07:30.325185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of happy overlap:  0\n",
      "len of sad overlap:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from tensorflow.keras.layers import Rescaling\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# splitting data\n",
    "happy_paths = os.listdir('smiley_faces_dataset/happy')\n",
    "sad_paths = os.listdir('smiley_faces_dataset/sad')\n",
    "\n",
    "train_happy_paths = random.sample(happy_paths, int(len(happy_paths)*0.8))\n",
    "test_happy_paths = [ p for p in happy_paths if p not in train_happy_paths]\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_happy_paths if p in test_happy_paths]\n",
    "print(\"len of happy overlap: \", len(overlap))\n",
    "\n",
    "\n",
    "train_sad_paths = random.sample(sad_paths, int(len(sad_paths)*0.8))\n",
    "test_sad_paths = [ p for p in sad_paths if p not in train_sad_paths]\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in train_sad_paths if p in test_sad_paths]\n",
    "print(\"len of sad overlap: \", len(overlap))\n",
    "\n",
    "# ensure to copy the images to the directories\n",
    "import shutil\n",
    "for p in train_happy_paths:\n",
    "    shutil.copyfile(os.path.join('smiley_faces_dataset/happy', p), os.path.join('smiley_faces_split/train/happy', p) )\n",
    "\n",
    "for p in test_happy_paths:\n",
    "    shutil.copyfile(os.path.join('smiley_faces_dataset/happy', p), os.path.join('smiley_faces_split/train/happy', p) )\n",
    "\n",
    "for p in train_sad_paths:\n",
    "    shutil.copyfile(os.path.join('smiley_faces_dataset/sad', p), os.path.join('smiley_faces_split/train/sad', p) )\n",
    "\n",
    "for p in test_sad_paths:\n",
    "    shutil.copyfile(os.path.join('smiley_faces_dataset/sad', p), os.path.join('smiley_faces_split/train/sad', p) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd55b17f-9b66-46ec-99d1-0bbd07299137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 216 files belonging to 2 classes.\n",
      "Using 173 files for training.\n",
      "Found 216 files belonging to 2 classes.\n",
      "Using 43 files for validation.\n"
     ]
    }
   ],
   "source": [
    "img_height = 143\n",
    "img_width = 107\n",
    "batch_size = 32\n",
    "train_data_dir = 'smiley_faces_split/train/'\n",
    "\n",
    "# preprocessing\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "train_data_dir,\n",
    "validation_split=0.2,\n",
    "subset=\"training\",\n",
    "seed=123,\n",
    "image_size=(img_height, img_width),\n",
    "batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "train_data_dir,\n",
    "validation_split=0.2,\n",
    "subset=\"validation\",\n",
    "seed=123,\n",
    "image_size=(img_height, img_width),\n",
    "batch_size=batch_size\n",
    ")\n",
    "\n",
    "\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "# Resize images to smaller dimensions'\n",
    "train_rescale_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_rescale_ds = val_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362c551d-afca-40c6-b1c0-184726929192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 1)),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),\n",
    "#     tf.keras.layers.Dense(2, activation='softmax')\n",
    "# ])\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# Intializing a sequential model\n",
    "model_cnn = models.Sequential()\n",
    "\n",
    "# Adding first conv layer with 64 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(img_height, img_width, 3)))\n",
    "\n",
    "# Adding max pooling to reduce the size of output of first conv layer\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding = 'same'))\n",
    "\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding = 'same'))\n",
    "\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2), padding = 'same'))\n",
    "\n",
    "# flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n",
    "model_cnn.add(layers.Flatten())\n",
    "\n",
    "# Adding a fully connected dense layer with 100 neurons\n",
    "model_cnn.add(layers.Dense(100, activation='relu'))\n",
    "\n",
    "# Adding a fully connected dense layer with 84 neurons\n",
    "model_cnn.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Adding the output layer with * neurons and activation functions as softmax since this is a multi-class classification problem\n",
    "model_cnn.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "# RMSprop (Root Mean Square Propagation) is commonly used in training deep neural networks.\n",
    "# model_cnn.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_cnn.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757ee059-8e29-4cf7-9f19-5d380ac786d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 143, 107, 3), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec = train_rescale_ds.element_spec\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de1c8c9-cbe6-492c-9a76-c8a4cf8525fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 143, 107, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 72, 54, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 72, 54, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 36, 27, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 36, 27, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 18, 14, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8064)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               806500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                8484      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 170       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 844658 (3.22 MB)\n",
      "Trainable params: 844658 (3.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2230239-28ab-462d-8828-2f3537d80568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6/6 [==============================] - 220s 340ms/step - loss: 0.7137 - accuracy: 0.4509 - val_loss: 0.7009 - val_accuracy: 0.4419\n",
      "Epoch 2/20\n",
      "6/6 [==============================] - 2s 287ms/step - loss: 0.6992 - accuracy: 0.4855 - val_loss: 0.6866 - val_accuracy: 0.5581\n",
      "Epoch 3/20\n",
      "6/6 [==============================] - 2s 276ms/step - loss: 0.6903 - accuracy: 0.5145 - val_loss: 0.6918 - val_accuracy: 0.4884\n",
      "Epoch 4/20\n",
      "6/6 [==============================] - 2s 232ms/step - loss: 0.6827 - accuracy: 0.6185 - val_loss: 0.6994 - val_accuracy: 0.4884\n",
      "Epoch 5/20\n",
      "6/6 [==============================] - 2s 273ms/step - loss: 0.6843 - accuracy: 0.4913 - val_loss: 0.6877 - val_accuracy: 0.5349\n",
      "Epoch 6/20\n",
      "6/6 [==============================] - 2s 274ms/step - loss: 0.6449 - accuracy: 0.6416 - val_loss: 0.7362 - val_accuracy: 0.5581\n",
      "Epoch 7/20\n",
      "6/6 [==============================] - 2s 333ms/step - loss: 0.6733 - accuracy: 0.5780 - val_loss: 0.6765 - val_accuracy: 0.6279\n",
      "Epoch 8/20\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 0.6082 - accuracy: 0.6647 - val_loss: 0.7814 - val_accuracy: 0.6047\n",
      "Epoch 9/20\n",
      "6/6 [==============================] - 2s 324ms/step - loss: 0.5665 - accuracy: 0.7341 - val_loss: 0.6893 - val_accuracy: 0.7674\n",
      "Epoch 10/20\n",
      "6/6 [==============================] - 2s 341ms/step - loss: 0.4501 - accuracy: 0.8266 - val_loss: 0.7395 - val_accuracy: 0.6977\n",
      "Epoch 11/20\n",
      "6/6 [==============================] - 2s 304ms/step - loss: 0.3682 - accuracy: 0.8671 - val_loss: 0.8995 - val_accuracy: 0.6279\n",
      "Epoch 12/20\n",
      "6/6 [==============================] - 2s 354ms/step - loss: 0.2953 - accuracy: 0.8960 - val_loss: 0.9945 - val_accuracy: 0.7209\n",
      "Epoch 13/20\n",
      "6/6 [==============================] - 2s 247ms/step - loss: 0.2372 - accuracy: 0.9306 - val_loss: 1.1953 - val_accuracy: 0.6512\n",
      "Epoch 14/20\n",
      "6/6 [==============================] - 2s 229ms/step - loss: 0.2334 - accuracy: 0.8960 - val_loss: 0.9116 - val_accuracy: 0.7674\n",
      "Epoch 15/20\n",
      "6/6 [==============================] - 2s 225ms/step - loss: 0.1738 - accuracy: 0.9364 - val_loss: 0.7890 - val_accuracy: 0.7442\n",
      "Epoch 16/20\n",
      "6/6 [==============================] - 1s 220ms/step - loss: 0.1559 - accuracy: 0.9480 - val_loss: 0.8532 - val_accuracy: 0.7907\n",
      "Epoch 17/20\n",
      "6/6 [==============================] - 2s 252ms/step - loss: 0.0883 - accuracy: 0.9769 - val_loss: 1.3327 - val_accuracy: 0.6744\n",
      "Epoch 18/20\n",
      "6/6 [==============================] - 2s 248ms/step - loss: 0.0666 - accuracy: 0.9711 - val_loss: 0.9344 - val_accuracy: 0.7209\n",
      "Epoch 19/20\n",
      "6/6 [==============================] - 2s 237ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 1.2783 - val_accuracy: 0.6977\n",
      "Epoch 20/20\n",
      "6/6 [==============================] - 2s 253ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 1.3055 - val_accuracy: 0.7442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f8a7dce5250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "model_cnn.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a17431-ee28-4e2d-98d6-6d446fd20254",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.save('models/cnn_first.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702492e7-884d-483b-9555-a954c50314bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad8f2e-2950-496a-887a-26091b1f807b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
